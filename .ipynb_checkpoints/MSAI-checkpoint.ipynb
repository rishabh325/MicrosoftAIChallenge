{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z16v3Aj8H37x"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0cFqKyUdJCMn"
   },
   "outputs": [],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TUU2Pej3Jkh4",
    "outputId": "c253b762-c07f-47d9-c533-8f191f107ed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " check.py   CNN   file.txt  'Starting Kit'\n"
     ]
    }
   ],
   "source": [
    "!ls drive/Rishabh\n",
    "#images001 = glob(os.path.join(\"drive/app/sample/sample/images\", \"*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wY-gYAUbSmyE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "from sklearn.cross_validation import train_test_split;\n",
    "print('loading...');\n",
    "data = pd.read_csv('drive/Rishabh/CNN/data.csv', sep='\\t');\n",
    "print('loaded');\n",
    "data = np.asarray(data);\n",
    "X = data[:, :-1];\n",
    "y = data[:, -1];\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=42);\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42FKCpuxeN4H"
   },
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0], 1);\n",
    "y_test = y_test.reshape(y_test.shape[0], 1);\n",
    "x1 = np.concatenate((X_train, y_train), axis=1);\n",
    "y1 = np.concatenate((X_test, y_test), axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JLyVk2-hzOcg"
   },
   "outputs": [],
   "source": [
    "import nltk;\n",
    "nltk.download('stopwords');\n",
    "nltk.download('punkt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T6eS2z271NjB"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize;\n",
    "from nltk.corpus import stopwords;\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "\n",
    "\n",
    "\n",
    "docIDFDict = {}\n",
    "avgDocLength = 0\n",
    "\n",
    "\n",
    "def GetCorpus(inputfile,corpusfile):\n",
    "\n",
    "\n",
    "\n",
    "    sw = set(stopwords.words('english'));\n",
    "    f = open(inputfile,\"r\",encoding=\"utf-8\")\n",
    "    fw = open(corpusfile,\"w\",encoding=\"utf-8\")\n",
    "    \n",
    "    \n",
    "    dw = pd.read_csv('drive/Rishabh/CNN/data.csv');\n",
    "    \n",
    "    dw = dw[:]\n",
    "  \n",
    "    i=0;\n",
    "    print(i);\n",
    "    strr = '';\n",
    "    for line in f:\n",
    "        passage = line.strip().lower().split(\"\\t\")[2]\n",
    "        tok = word_tokenize(passage);\n",
    "        filtered = [];\n",
    "        for w in tok:\n",
    "            if not w in sw:\n",
    "                filtered.append(w);\n",
    "        strr =' '.join(filtered);\n",
    "#         fw.write(strr+'\\n');\n",
    "        i = i+1;\n",
    "        if i%1000==0:\n",
    "#             fw.write(strr+'\\n');\n",
    "#             strr = '';\n",
    "            print(i);\n",
    "\n",
    "    f.close()\n",
    "    fw.close()\n",
    "\n",
    "\n",
    "\n",
    "# The following IDF_Generator method reads all the passages(docs) and creates Inverse Document Frequency(IDF) scores for each unique word using below formula \n",
    "# IDF(q_i) = log((N-n(q_i)+0.5)/(n(q_i)+0.5)) where N is the total number of documents in the collection and n(q_i) is the number of documents containing q_i\n",
    "# After finding IDF scores for all the words, The IDF dictionary will be saved in \"docIDFDict.pickle\" file in the current directory\n",
    "\n",
    "def IDF_Generator(corpusfile, delimiter=' ', base=math.e) :\n",
    "\n",
    "    global docIDFDict,avgDocLength\n",
    "\n",
    "    docFrequencyDict = {}       \n",
    "    numOfDocuments = 0   \n",
    "    totalDocLength = 0\n",
    "\n",
    "    for line in open(corpusfile,\"r\",encoding=\"utf-8\") :\n",
    "        doc = line.strip().split(delimiter)\n",
    "        totalDocLength += len(doc)\n",
    "\n",
    "        doc = list(set(doc)) # Take all unique words\n",
    "\n",
    "        mx = 0;\n",
    "        \n",
    "        for word in doc : #Updates n(q_i) values for all the words(q_i)\n",
    "            if word not in docFrequencyDict :\n",
    "                docFrequencyDict[word] = 0\n",
    "            docFrequencyDict[word] += 1\n",
    "            \n",
    "            mx = max(docFrequencyDict[word], mx);\n",
    "        \n",
    "        \n",
    "          \n",
    "        for word in doc:\n",
    "          docFrequencyDict[word] = 0.5 + (docFrequencyDict[word]/mx)*0.5; \n",
    "          \n",
    "\n",
    "        numOfDocuments = numOfDocuments + 1\n",
    "                    \n",
    "\n",
    "    for word in docFrequencyDict:  #Calculate IDF scores for each word(q_i)\n",
    "        docIDFDict[word] = math.log((numOfDocuments + 1) / (docFrequencyDict[word]), base)\n",
    "        \n",
    "    avgDocLength = totalDocLength / numOfDocuments\n",
    "\n",
    "     \n",
    "#     pickle_out = open(\"drive/Rishabh/CNN/docIDFDict_stop.pickle\",\"wb\") # Saves IDF scores in pickle file, which is optional\n",
    "#     pickle.dump(docIDFDict, pickle_out)\n",
    "#     pickle_out.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#The following GetBM25Score method will take Query and passage as input and outputs their similarity score based on the term frequency(TF) and IDF values.\n",
    "# stop word removal; stemming; bm25+(b = 0.6, k=1.1, del = 0.6); double normalised tf\n",
    "def GetBM25Score(Query, Passage, k1=1.1, b=0.6, delimiter=' ') :\n",
    "    \n",
    "    global docIDFDict,avgDocLength\n",
    "\n",
    "    query_words= Query.strip().lower().split(delimiter)\n",
    "    passage_words = Passage.strip().lower().split(delimiter)\n",
    "    passageLen = len(passage_words)\n",
    "    docTF = {}\n",
    "    for word in set(query_words):   #Find Term Frequency of all query unique words\n",
    "        docTF[word] = passage_words.count(word)\n",
    "    commonWords = set(query_words) & set(passage_words)\n",
    "    tmp_score = []\n",
    "    for word in commonWords :   \n",
    "        numer = (docTF[word] * (k1+1))   #Numerator part of BM25 Formula\n",
    "        denom = ((docTF[word]) + k1*(1 - b + b*passageLen/avgDocLength)) #Denominator part of BM25 Formula \n",
    "        \n",
    "#         numer1 = docTF[word];\n",
    "#         denom1 = 1 - b + b*(passageLen/avgDocLength);\n",
    "        \n",
    "#         tm1 = 1 + math.log(math.log((numer1/denom1)+1)+1);\n",
    "        \n",
    "        if(word in docIDFDict) :\n",
    "#             tmp_score.append(docIDFDict[word] * tm1);\n",
    "              tmp_score.append(docIDFDict[word] * ((numer/denom)+0.6));\n",
    "\n",
    "    score = sum(tmp_score)\n",
    "    return score\n",
    "\n",
    "#The following line reads each line from testfile and extracts query, passage and calculates BM25 similarity scores and writes the output in outputfile\n",
    "def RunBM25OnEvaluationSet(testfile,outputfile):\n",
    "\n",
    "    lno=0\n",
    "    tempscores=[]  #This will store scores of 10 query,passage pairs as they belong to same query\n",
    "    f = open(testfile,\"r\",encoding=\"utf-8\")\n",
    "    fw = open(outputfile,\"w\",encoding=\"utf-8\")\n",
    "    sw = stopwords.words('english');\n",
    "    for line in f:\n",
    "        tokens = line.strip().lower().split(\"\\t\")\n",
    "        \n",
    "        Query = tokens[1];\n",
    "        tok = word_tokenize(Query);\n",
    "        fil = [stemmer.stem(w) for w in tok if not w in sw]\n",
    "        Query =  ' '.join(fil)\n",
    "        \n",
    "        \n",
    "        Passage = tokens[2];\n",
    "        tok = word_tokenize(Passage);\n",
    "        fil = [stemmer.stem(w) for w in tok if not w in sw]\n",
    "        Passage = ' '.join(fil);\n",
    "        \n",
    "        score = GetBM25Score(Query,Passage) \n",
    "        tempscores.append(score)\n",
    "        lno+=1\n",
    "        if(lno%10==0):\n",
    "            tempscores = [str(s) for s in tempscores]\n",
    "            scoreString = \"\\t\".join(tempscores)\n",
    "            qid = tokens[0]\n",
    "            fw.write(qid+\"\\t\"+scoreString+\"\\n\")\n",
    "            tempscores=[]\n",
    "        if(lno%5000==0):\n",
    "            print(lno)\n",
    "    f.close()\n",
    "    fw.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "\n",
    "    #inputFileName = \"drive/Rishabh/CNN/data.csv\"   # This file should be in the following format : queryid \\t query \\t passage \\t label \\t passageid\n",
    "    testFileName = \"drive/Rishabh/CNN/eval2_unlabelled.tsv\"  # This file should be in the following format : queryid \\t query \\t passage \\t passageid # order of the query\n",
    "    corpusFileName = \"drive/Rishabh/CNN/stopWordRemovedStemmed.csv\" \n",
    "    outputFileName = \"drive/Rishabh/CNN/answerPhase2_1.tsv\"\n",
    "\n",
    "    #GetCorpus(inputFileName,corpusFileName)    # Gets all the passages(docs) and stores in corpusFile. you can comment this line if corpus file is already generated\n",
    "    print(\"Corpus File is created.\")\n",
    "    IDF_Generator(corpusFileName)   # Calculates IDF scores. \n",
    "    print(\"IDF Dictionary Generated.\")\n",
    "    RunBM25OnEvaluationSet(testFileName,outputFileName)\n",
    "    print(\"Submission file created. \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0DLTt2kdGaK8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "dw = pd.read_csv('drive/Rishabh/CNN/sampledData.csv', delimiter='\\t', header=None);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zw-LVpAu26mB"
   },
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "from nltk.corpus import stopwords;\n",
    "from nltk.tokenize import word_tokenize;\n",
    "\n",
    "dw = np.asarray(dw)[:,2];\n",
    "\n",
    "sw = stopwords.words('english');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D06QHT8V-qza",
    "outputId": "17b612cd-09b9-4a46-f795-ad9b68feceb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2096752, 5)"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndoK3gn23T20"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3hn5hNWKUnB"
   },
   "outputs": [],
   "source": [
    "i=0;\n",
    "for line in dw:\n",
    "  tok = word_tokenize(line);\n",
    "  \n",
    "  fil = [stemmer.stem(w) for w in tok if not w in sw]\n",
    "  fil =  ' '.join(fil)\n",
    "  \n",
    "  dw[i] = fil;\n",
    "  i = i+1;\n",
    "  if i%10000==0:\n",
    "    print(i);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7MRk6Z8ZUwWb",
    "outputId": "9e179278-1088-4894-971a-7fa076c6883f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2096752,)"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OS282bDYa8uc"
   },
   "outputs": [],
   "source": [
    "np.savetxt('drive/Rishabh/CNN/stopWordRemovedStemmedSampled.csv', dw, fmt='%s');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D5HMENvgbpwR"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('drive/Rishabh/CNN/stopWordRemovedStemmedSampled.csv', delimiter='\\n', header=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QscD1OM3WSUZ"
   },
   "outputs": [],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "yE7SMPzLdHFJ",
    "outputId": "fcd5204b-cfa2-42e7-c2b3-031ac2fa2ebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5241880, 5)\n",
      "read\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import random;\n",
    "\n",
    "data = pd.read_csv('drive/Rishabh/CNN/data.csv', sep='\\t', header=None);\n",
    "print(data.shape);\n",
    "print('read');\n",
    "npd = np.asarray(data);\n",
    "\n",
    "rows = npd.shape[0];\n",
    "\n",
    "cnt=0;\n",
    "i=0;\n",
    "nnpd = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqNM8VJVeI6N"
   },
   "outputs": [],
   "source": [
    "while i < rows:\n",
    "  if i%1000000==0:\n",
    "    print(i);\n",
    "  neg = [npd[i+w] for w in range(10) if not npd[i+w][3]==1]\n",
    "# \tneg = [npd[i+w] for w in range(10) if not npd[i+w][3]==1]\n",
    "  rnd = random.sample(neg, 3);\n",
    "# \trnd = random.sample(neg, 3);\n",
    "  j = i;\n",
    "  while j<rows :\n",
    "    if npd[j][3]==1:\n",
    "      pos = npd[j];\n",
    "      break;\n",
    "    j = j+1;\n",
    "  i = i+10;\n",
    "  rnd.append(pos);\n",
    "  nnpd.append(rnd);\n",
    "nnpd = np.asarray(nnpd);\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MSAI.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
