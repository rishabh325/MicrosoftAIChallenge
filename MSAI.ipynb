{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSAI.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Z16v3Aj8H37x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0cFqKyUdJCMn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TUU2Pej3Jkh4",
        "colab_type": "code",
        "outputId": "c253b762-c07f-47d9-c533-8f191f107ed6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls drive/Akshay\n",
        "#images001 = glob(os.path.join(\"drive/app/sample/sample/images\", \"*.png\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " check.py   CNN   file.txt  'Starting Kit'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wY-gYAUbSmyE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd;\n",
        "import numpy as np;\n",
        "from sklearn.cross_validation import train_test_split;\n",
        "print('loading...');\n",
        "data = pd.read_csv('drive/Akshay/CNN/data.csv', sep='\\t');\n",
        "print('loaded');\n",
        "data = np.asarray(data);\n",
        "X = data[:, :-1];\n",
        "y = data[:, -1];\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=42);\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "42FKCpuxeN4H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train = y_train.reshape(y_train.shape[0], 1);\n",
        "y_test = y_test.reshape(y_test.shape[0], 1);\n",
        "x1 = np.concatenate((X_train, y_train), axis=1);\n",
        "y1 = np.concatenate((X_test, y_test), axis=1);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JLyVk2-hzOcg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk;\n",
        "nltk.download('stopwords');\n",
        "nltk.download('punkt');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T6eS2z271NjB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pickle\n",
        "from nltk.tokenize import word_tokenize;\n",
        "from nltk.corpus import stopwords;\n",
        "import pandas as pd;\n",
        "import numpy as np;\n",
        "\n",
        "\n",
        "\n",
        "docIDFDict = {}\n",
        "avgDocLength = 0\n",
        "\n",
        "\n",
        "def GetCorpus(inputfile,corpusfile):\n",
        "\n",
        "\n",
        "\n",
        "    sw = set(stopwords.words('english'));\n",
        "    f = open(inputfile,\"r\",encoding=\"utf-8\")\n",
        "    fw = open(corpusfile,\"w\",encoding=\"utf-8\")\n",
        "    \n",
        "    \n",
        "    dw = pd.read_csv('drive/Akshay/CNN/data.csv');\n",
        "    \n",
        "    dw = dw[:]\n",
        "  \n",
        "    i=0;\n",
        "    print(i);\n",
        "    strr = '';\n",
        "    for line in f:\n",
        "        passage = line.strip().lower().split(\"\\t\")[2]\n",
        "        tok = word_tokenize(passage);\n",
        "        filtered = [];\n",
        "        for w in tok:\n",
        "            if not w in sw:\n",
        "                filtered.append(w);\n",
        "        strr =' '.join(filtered);\n",
        "#         fw.write(strr+'\\n');\n",
        "        i = i+1;\n",
        "        if i%1000==0:\n",
        "#             fw.write(strr+'\\n');\n",
        "#             strr = '';\n",
        "            print(i);\n",
        "\n",
        "    f.close()\n",
        "    fw.close()\n",
        "\n",
        "\n",
        "\n",
        "# The following IDF_Generator method reads all the passages(docs) and creates Inverse Document Frequency(IDF) scores for each unique word using below formula \n",
        "# IDF(q_i) = log((N-n(q_i)+0.5)/(n(q_i)+0.5)) where N is the total number of documents in the collection and n(q_i) is the number of documents containing q_i\n",
        "# After finding IDF scores for all the words, The IDF dictionary will be saved in \"docIDFDict.pickle\" file in the current directory\n",
        "\n",
        "def IDF_Generator(corpusfile, delimiter=' ', base=math.e) :\n",
        "\n",
        "    global docIDFDict,avgDocLength\n",
        "\n",
        "    docFrequencyDict = {}       \n",
        "    numOfDocuments = 0   \n",
        "    totalDocLength = 0\n",
        "\n",
        "    for line in open(corpusfile,\"r\",encoding=\"utf-8\") :\n",
        "        doc = line.strip().split(delimiter)\n",
        "        totalDocLength += len(doc)\n",
        "\n",
        "        doc = list(set(doc)) # Take all unique words\n",
        "\n",
        "        mx = 0;\n",
        "        \n",
        "        for word in doc : #Updates n(q_i) values for all the words(q_i)\n",
        "            if word not in docFrequencyDict :\n",
        "                docFrequencyDict[word] = 0\n",
        "            docFrequencyDict[word] += 1\n",
        "            \n",
        "            mx = max(docFrequencyDict[word], mx);\n",
        "        \n",
        "        \n",
        "          \n",
        "        for word in doc:\n",
        "          docFrequencyDict[word] = 0.5 + (docFrequencyDict[word]/mx)*0.5; \n",
        "          \n",
        "\n",
        "        numOfDocuments = numOfDocuments + 1\n",
        "                    \n",
        "\n",
        "    for word in docFrequencyDict:  #Calculate IDF scores for each word(q_i)\n",
        "        docIDFDict[word] = math.log((numOfDocuments + 1) / (docFrequencyDict[word]), base)\n",
        "        \n",
        "    avgDocLength = totalDocLength / numOfDocuments\n",
        "\n",
        "     \n",
        "#     pickle_out = open(\"drive/Akshay/CNN/docIDFDict_stop.pickle\",\"wb\") # Saves IDF scores in pickle file, which is optional\n",
        "#     pickle.dump(docIDFDict, pickle_out)\n",
        "#     pickle_out.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#The following GetBM25Score method will take Query and passage as input and outputs their similarity score based on the term frequency(TF) and IDF values.\n",
        "# stop word removal; stemming; bm25+(b = 0.6, k=1.1, del = 0.6); double normalised tf\n",
        "def GetBM25Score(Query, Passage, k1=1.1, b=0.6, delimiter=' ') :\n",
        "    \n",
        "    global docIDFDict,avgDocLength\n",
        "\n",
        "    query_words= Query.strip().lower().split(delimiter)\n",
        "    passage_words = Passage.strip().lower().split(delimiter)\n",
        "    passageLen = len(passage_words)\n",
        "    docTF = {}\n",
        "    for word in set(query_words):   #Find Term Frequency of all query unique words\n",
        "        docTF[word] = passage_words.count(word)\n",
        "    commonWords = set(query_words) & set(passage_words)\n",
        "    tmp_score = []\n",
        "    for word in commonWords :   \n",
        "        numer = (docTF[word] * (k1+1))   #Numerator part of BM25 Formula\n",
        "        denom = ((docTF[word]) + k1*(1 - b + b*passageLen/avgDocLength)) #Denominator part of BM25 Formula \n",
        "        \n",
        "#         numer1 = docTF[word];\n",
        "#         denom1 = 1 - b + b*(passageLen/avgDocLength);\n",
        "        \n",
        "#         tm1 = 1 + math.log(math.log((numer1/denom1)+1)+1);\n",
        "        \n",
        "        if(word in docIDFDict) :\n",
        "#             tmp_score.append(docIDFDict[word] * tm1);\n",
        "              tmp_score.append(docIDFDict[word] * ((numer/denom)+0.6));\n",
        "\n",
        "    score = sum(tmp_score)\n",
        "    return score\n",
        "\n",
        "#The following line reads each line from testfile and extracts query, passage and calculates BM25 similarity scores and writes the output in outputfile\n",
        "def RunBM25OnEvaluationSet(testfile,outputfile):\n",
        "\n",
        "    lno=0\n",
        "    tempscores=[]  #This will store scores of 10 query,passage pairs as they belong to same query\n",
        "    f = open(testfile,\"r\",encoding=\"utf-8\")\n",
        "    fw = open(outputfile,\"w\",encoding=\"utf-8\")\n",
        "    sw = stopwords.words('english');\n",
        "    for line in f:\n",
        "        tokens = line.strip().lower().split(\"\\t\")\n",
        "        \n",
        "        Query = tokens[1];\n",
        "        tok = word_tokenize(Query);\n",
        "        fil = [stemmer.stem(w) for w in tok if not w in sw]\n",
        "        Query =  ' '.join(fil)\n",
        "        \n",
        "        \n",
        "        Passage = tokens[2];\n",
        "        tok = word_tokenize(Passage);\n",
        "        fil = [stemmer.stem(w) for w in tok if not w in sw]\n",
        "        Passage = ' '.join(fil);\n",
        "        \n",
        "        score = GetBM25Score(Query,Passage) \n",
        "        tempscores.append(score)\n",
        "        lno+=1\n",
        "        if(lno%10==0):\n",
        "            tempscores = [str(s) for s in tempscores]\n",
        "            scoreString = \"\\t\".join(tempscores)\n",
        "            qid = tokens[0]\n",
        "            fw.write(qid+\"\\t\"+scoreString+\"\\n\")\n",
        "            tempscores=[]\n",
        "        if(lno%5000==0):\n",
        "            print(lno)\n",
        "    f.close()\n",
        "    fw.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__' :\n",
        "\n",
        "    #inputFileName = \"drive/Akshay/CNN/data.csv\"   # This file should be in the following format : queryid \\t query \\t passage \\t label \\t passageid\n",
        "    testFileName = \"drive/Akshay/CNN/eval2_unlabelled.tsv\"  # This file should be in the following format : queryid \\t query \\t passage \\t passageid # order of the query\n",
        "    corpusFileName = \"drive/Akshay/CNN/stopWordRemovedStemmed.csv\" \n",
        "    outputFileName = \"drive/Akshay/CNN/answerPhase2_1.tsv\"\n",
        "\n",
        "    #GetCorpus(inputFileName,corpusFileName)    # Gets all the passages(docs) and stores in corpusFile. you can comment this line if corpus file is already generated\n",
        "    print(\"Corpus File is created.\")\n",
        "    IDF_Generator(corpusFileName)   # Calculates IDF scores. \n",
        "    print(\"IDF Dictionary Generated.\")\n",
        "    RunBM25OnEvaluationSet(testFileName,outputFileName)\n",
        "    print(\"Submission file created. \")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0DLTt2kdGaK8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd;\n",
        "dw = pd.read_csv('drive/Akshay/CNN/sampledData.csv', delimiter='\\t', header=None);\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zw-LVpAu26mB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np;\n",
        "from nltk.corpus import stopwords;\n",
        "from nltk.tokenize import word_tokenize;\n",
        "\n",
        "dw = np.asarray(dw)[:,2];\n",
        "\n",
        "sw = stopwords.words('english');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D06QHT8V-qza",
        "colab_type": "code",
        "outputId": "17b612cd-09b9-4a46-f795-ad9b68feceb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "dw.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2096752, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "ndoK3gn23T20",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t3hn5hNWKUnB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i=0;\n",
        "for line in dw:\n",
        "  tok = word_tokenize(line);\n",
        "  \n",
        "  fil = [stemmer.stem(w) for w in tok if not w in sw]\n",
        "  fil =  ' '.join(fil)\n",
        "  \n",
        "  dw[i] = fil;\n",
        "  i = i+1;\n",
        "  if i%10000==0:\n",
        "    print(i);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7MRk6Z8ZUwWb",
        "colab_type": "code",
        "outputId": "9e179278-1088-4894-971a-7fa076c6883f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "dw.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2096752,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "OS282bDYa8uc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.savetxt('drive/Akshay/CNN/stopWordRemovedStemmedSampled.csv', dw, fmt='%s');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D5HMENvgbpwR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('drive/Akshay/CNN/stopWordRemovedStemmedSampled.csv', delimiter='\\n', header=None);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QscD1OM3WSUZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.head"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yE7SMPzLdHFJ",
        "colab_type": "code",
        "outputId": "fcd5204b-cfa2-42e7-c2b3-031ac2fa2ebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd;\n",
        "import numpy as np;\n",
        "import random;\n",
        "\n",
        "data = pd.read_csv('drive/Akshay/CNN/data.csv', sep='\\t', header=None);\n",
        "print(data.shape);\n",
        "print('read');\n",
        "npd = np.asarray(data);\n",
        "\n",
        "rows = npd.shape[0];\n",
        "\n",
        "cnt=0;\n",
        "i=0;\n",
        "nnpd = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5241880, 5)\n",
            "read\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lqNM8VJVeI6N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "while i < rows:\n",
        "  if i%1000000==0:\n",
        "    print(i);\n",
        "  neg = [npd[i+w] for w in range(10) if not npd[i+w][3]==1]\n",
        "# \tneg = [npd[i+w] for w in range(10) if not npd[i+w][3]==1]\n",
        "  rnd = random.sample(neg, 3);\n",
        "# \trnd = random.sample(neg, 3);\n",
        "  j = i;\n",
        "  while j<rows :\n",
        "    if npd[j][3]==1:\n",
        "      pos = npd[j];\n",
        "      break;\n",
        "    j = j+1;\n",
        "  i = i+10;\n",
        "  rnd.append(pos);\n",
        "  nnpd.append(rnd);\n",
        "nnpd = np.asarray(nnpd);\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}